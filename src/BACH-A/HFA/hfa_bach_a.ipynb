{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5045636,"sourceType":"datasetVersion","datasetId":2929186},{"sourceId":9611120,"sourceType":"datasetVersion","datasetId":5581965}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# importing necessary libraries \n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nimport torchvision \nfrom torchvision import transforms, datasets, models \nfrom torch.nn import functional as F \nfrom PIL import Image\nimport pandas as pd \nimport numpy as np\nimport tensorflow as tf\nimport os\nimport sys\nimport glob\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt \nimport cv2\nimport json \nfrom tqdm import tqdm\nfrom sklearn.decomposition import PCA \nfrom sklearn.preprocessing import StandardScaler\nfrom skimage.transform import rotate, AffineTransform\nimport random\nfrom scipy import ndimage\nimport openslide\nimport matplotlib.patches as patches\nfrom matplotlib.patches import Polygon\nimport xml.etree.ElementTree as ET \nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix","metadata":{"execution":{"iopub.status.busy":"2024-09-12T15:41:21.446943Z","iopub.execute_input":"2024-09-12T15:41:21.447315Z","iopub.status.idle":"2024-09-12T15:41:21.455819Z","shell.execute_reply.started":"2024-09-12T15:41:21.447288Z","shell.execute_reply":"2024-09-12T15:41:21.454912Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**EXTRACTING PATCH EMBEDDINGS** ","metadata":{}},{"cell_type":"code","source":"# feature extractor for patches \n\nclass ViT(nn.Module):\n    def __init__(self,num_classes):\n        #define necessary layers\n        super().__init__()\n        self.num_classes = num_classes\n        self.model = models.vit_b_32(weights='DEFAULT')\n        \n        # Unfreeze model weights\n        for param in self.model.parameters():\n            param.requires_grad = False \n        \n    def forward(self,X):\n        #define forward pass here\n        X = self.model(X)\n        return X        \n            \nmodel = ViT(1)\nmodel = model.to('cuda')","metadata":{"execution":{"iopub.status.busy":"2024-09-12T15:41:22.378988Z","iopub.execute_input":"2024-09-12T15:41:22.379650Z","iopub.status.idle":"2024-09-12T15:41:22.945220Z","shell.execute_reply.started":"2024-09-12T15:41:22.379620Z","shell.execute_reply":"2024-09-12T15:41:22.944409Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# transform function for patches \n\ntransform = torchvision.transforms.Compose(\n    [ \n        torchvision.transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),  # normalization\n    ] \n) ","metadata":{"execution":{"iopub.status.busy":"2024-09-12T15:41:23.459823Z","iopub.execute_input":"2024-09-12T15:41:23.460549Z","iopub.status.idle":"2024-09-12T15:41:23.465464Z","shell.execute_reply.started":"2024-09-12T15:41:23.460519Z","shell.execute_reply":"2024-09-12T15:41:23.464466Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# function to obtain feature embedding for a given patch \n\ndef get_feature_vector(img): \n    img = torch.from_numpy(img.astype(np.double)) \n    img = img.to('cuda')\n    img = img.permute(2, 0, 1) \n    img = transform(img) \n    img = img.float() \n    img = torch.unsqueeze(img, dim=0) \n    return model(img) ","metadata":{"execution":{"iopub.status.busy":"2024-09-12T15:41:25.624777Z","iopub.execute_input":"2024-09-12T15:41:25.625651Z","iopub.status.idle":"2024-09-12T15:41:25.631079Z","shell.execute_reply.started":"2024-09-12T15:41:25.625617Z","shell.execute_reply":"2024-09-12T15:41:25.630077Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get a list of all image ids\n\nbenign_ids = os.listdir('/kaggle/input/bach-breast-cancer-histology-images/ICIAR2018_BACH_Challenge/ICIAR2018_BACH_Challenge/Photos/Benign')\ninsitu_ids = os.listdir('/kaggle/input/bach-breast-cancer-histology-images/ICIAR2018_BACH_Challenge/ICIAR2018_BACH_Challenge/Photos/InSitu')\ninvasive_ids = os.listdir('/kaggle/input/bach-breast-cancer-histology-images/ICIAR2018_BACH_Challenge/ICIAR2018_BACH_Challenge/Photos/Invasive')\nnormal_ids = os.listdir('/kaggle/input/bach-breast-cancer-histology-images/ICIAR2018_BACH_Challenge/ICIAR2018_BACH_Challenge/Photos/Normal')","metadata":{"execution":{"iopub.status.busy":"2024-09-12T15:41:28.471152Z","iopub.execute_input":"2024-09-12T15:41:28.471904Z","iopub.status.idle":"2024-09-12T15:41:28.481317Z","shell.execute_reply.started":"2024-09-12T15:41:28.471866Z","shell.execute_reply":"2024-09-12T15:41:28.480535Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Patch Embedding Extraction\n\nThis function `get_patch_embeddings` is used to extract feature vectors (embeddings) from patches of histology images. Each patch is extracted from a larger image, and its corresponding feature vector is computed. The function also generates one-hot encoded labels for each image based on its class (Benign, InSitu, Invasive, or Normal).","metadata":{}},{"cell_type":"code","source":"def get_patch_embeddings(img_ids, label): \n    '''\n    Function to return feature vectors along with labels for patches.\n    Inputs:\n    - img_ids: List\n    - label: String\n    Returns:\n    - feature_vectors: List\n    - labels: List\n    '''\n    feature_vectors = [] \n    labels = [] \n    \n    for img_id in tqdm(img_ids):         \n        if img_id.endswith('tif'):\n            # Path to the TIF file\n            img_path = '/kaggle/input/bach-breast-cancer-histology-images/ICIAR2018_BACH_Challenge/ICIAR2018_BACH_Challenge/Photos/' + label + '/' + img_id\n            img = cv2.imread(img_path)\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) \n\n            # Check if the image was read successfully\n            if img is not None:\n                #print(img.shape)\n                pass\n            else:\n                print(\"Error: Could not read the image.\")\n\n            width, height, channels = img.shape\n\n            patch_no = 0\n\n            # loop to slide vertically for patches \n            for i in range(int(0), int(height), 112): \n                # loop to slide horizontally \n                for j in range(int(0), int(width), 112): \n\n                    # Read a region of the slide - current patch\n                    curr_patch = np.array(img) \n                    curr_patch = curr_patch[i:i+224, j:j+224]\n\n                    if curr_patch.shape != (224, 224, 3):\n                        continue\n                    patch_no += 1\n                    \n                    # storing feature vector\n                    feature_vector = get_feature_vector(curr_patch) \n                    squeezed_arr = np.squeeze(feature_vector) \n\n                    feature_vectors.append(squeezed_arr) \n\n                    # Storing one hot encoding labels \n                    labels.append([float(label=='Benign'), float(label=='InSitu'),float(label=='Invasive'), float(label=='Normal')]) \n\n    return feature_vectors, labels","metadata":{"execution":{"iopub.status.busy":"2024-09-12T15:41:30.672513Z","iopub.execute_input":"2024-09-12T15:41:30.673348Z","iopub.status.idle":"2024-09-12T15:41:30.683407Z","shell.execute_reply.started":"2024-09-12T15:41:30.673313Z","shell.execute_reply":"2024-09-12T15:41:30.682436Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# obtaining patch embeddings for all wsis \n\nbenign_patch_embeddings, benign_patch_labels = get_patch_embeddings(benign_ids, 'Benign') \ninsitu_patch_embeddings, insitu_patch_labels = get_patch_embeddings(insitu_ids, 'InSitu') \ninvasive_patch_embeddings, invasive_patch_labels= get_patch_embeddings(invasive_ids, 'Invasive')\nnormal_patch_embeddings, normal_patch_labels = get_patch_embeddings(normal_ids, 'Normal') ","metadata":{"execution":{"iopub.status.busy":"2024-09-12T15:41:31.347341Z","iopub.execute_input":"2024-09-12T15:41:31.347682Z","iopub.status.idle":"2024-09-12T15:54:37.977525Z","shell.execute_reply.started":"2024-09-12T15:41:31.347657Z","shell.execute_reply":"2024-09-12T15:54:37.976538Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# function to apply PCA to reduce dimensions to 500 \n\ndef getPCA(feature_map, n):\n    pca = PCA(n_components = n)\n    feature_map = [f.cpu().detach() for f in feature_map]\n    pca.fit(feature_map)\n    return pca.transform(feature_map) ","metadata":{"execution":{"iopub.status.busy":"2024-09-12T15:54:49.508931Z","iopub.execute_input":"2024-09-12T15:54:49.509590Z","iopub.status.idle":"2024-09-12T15:54:49.514684Z","shell.execute_reply.started":"2024-09-12T15:54:49.509559Z","shell.execute_reply":"2024-09-12T15:54:49.513754Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# applying PCA \n\nbenign_features = getPCA(benign_patch_embeddings, 224) \ninsitu_features = getPCA(insitu_patch_embeddings, 224) \ninvasive_features = getPCA(invasive_patch_embeddings, 224) \nnormal_features = getPCA(normal_patch_embeddings, 224)","metadata":{"execution":{"iopub.status.busy":"2024-09-12T15:54:51.003749Z","iopub.execute_input":"2024-09-12T15:54:51.004584Z","iopub.status.idle":"2024-09-12T15:55:03.845601Z","shell.execute_reply.started":"2024-09-12T15:54:51.004550Z","shell.execute_reply":"2024-09-12T15:55:03.844120Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**BAG FORMATION** ","metadata":{}},{"cell_type":"code","source":"def create_bags(img_ids, feature_maps, labels, no_of_patches):\n    \n    i = 0\n    bags = []\n    bag_labels = []\n    img_ids_against_bags = []\n    \n    for img in img_ids:\n        if img.endswith('.tif'):\n            bags.append(feature_maps[i: i + no_of_patches])\n            bag_labels.append(labels[i])   \n            img_ids_against_bags.append(img) \n            i += no_of_patches\n    \n    return bags, bag_labels, img_ids_against_bags","metadata":{"execution":{"iopub.status.busy":"2024-09-12T16:12:36.804517Z","iopub.execute_input":"2024-09-12T16:12:36.804921Z","iopub.status.idle":"2024-09-12T16:12:36.813900Z","shell.execute_reply.started":"2024-09-12T16:12:36.804885Z","shell.execute_reply":"2024-09-12T16:12:36.812742Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# creating bags \n\nbenign_bags, benign_bag_labels, benign_ids_against_bags = create_bags(benign_ids, benign_features, benign_patch_labels, 168) \ninsitu_bags, insitu_bag_labels, insitu_ids_against_bags = create_bags(insitu_ids, insitu_features, insitu_patch_labels, 168) \ninvasive_bags, invasive_bag_labels, invasive_ids_against_bags = create_bags(invasive_ids, invasive_features, invasive_patch_labels, 168) \nnormal_bags, normal_bag_labels, normal_ids_against_bags = create_bags(normal_ids, normal_features, normal_patch_labels, 168) ","metadata":{"execution":{"iopub.status.busy":"2024-09-12T16:12:39.239470Z","iopub.execute_input":"2024-09-12T16:12:39.240270Z","iopub.status.idle":"2024-09-12T16:12:39.246546Z","shell.execute_reply.started":"2024-09-12T16:12:39.240232Z","shell.execute_reply":"2024-09-12T16:12:39.245619Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train, Test, and Validation Split\n\nThis code splits the dataset into train, validation, and test sets in a sequential manner. Data is first organized by class (benign, insitu, invasive, and normal), and then split ratio follows an 80-10-10 rule.","metadata":{}},{"cell_type":"code","source":"# Creating train, test and validation spilt\n\nbags = []\nlabels = []\nids_against_bags = []\n\nfor i in range(len(benign_bags)):  # Assuming all classes have the same length\n    bags.extend([benign_bags[i], insitu_bags[i], invasive_bags[i], normal_bags[i]])\n    labels.extend([benign_bag_labels[i], insitu_bag_labels[i], invasive_bag_labels[i], normal_bag_labels[i]])\n    ids_against_bags.extend([benign_ids_against_bags[i], insitu_ids_against_bags[i], invasive_ids_against_bags[i], normal_ids_against_bags[i]])\n\n# Manual split: 80% train, 10% validation, 10% test\nn_total = len(bags)\nn_train = int(0.8 * n_total)\nn_val = int(0.1 * n_total)\n\n# Train set\ntrain_bags = bags[:n_train]\ntrain_labels = labels[:n_train]\ntrain_img_ids_against_bag = ids_against_bags[:n_train]\n                             \n# Validation set\nval_bags = bags[n_train:n_train + n_val]\nval_labels = labels[n_train:n_train + n_val]\nval_img_ids_against_bag = ids_against_bags[:n_train + n_val]\n\n# Test set\ntest_bags = bags[n_train + n_val:]\ntest_labels = labels[n_train + n_val:]\ntest_img_ids_against_bag = ids_against_bags[n_train + n_val:]","metadata":{"execution":{"iopub.status.busy":"2024-09-12T16:13:11.867957Z","iopub.execute_input":"2024-09-12T16:13:11.868711Z","iopub.status.idle":"2024-09-12T16:13:11.877998Z","shell.execute_reply.started":"2024-09-12T16:13:11.868680Z","shell.execute_reply":"2024-09-12T16:13:11.876991Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# class distribution of train, validation, and test datasets \n\ndef plot_bag_labels(ax, title, labels):\n    # Convert list of labels (one-hot encoded) to a list of class indices\n    class_indices = [label.index(1) for label in labels]\n    \n    # Count occurrences of each class\n    class_counts = [class_indices.count(i) for i in range(4)]\n    \n    # Plot the distribution\n    ax.bar(['Benign', 'Insitu Carcinoma', 'Invasive Carcinoma', 'Normal'], class_counts)\n    ax.set_title(title)\n    ax.set_ylabel('Count')\n\n# Plotting\nfig, axs = plt.subplots(1, 3, figsize=(15, 5))\n\nplot_bag_labels(axs[0], 'TRAIN BAG LABELS', train_labels)\nplot_bag_labels(axs[1], 'VALIDATION BAG LABELS', val_labels)\nplot_bag_labels(axs[2], 'TEST BAG LABELS', test_labels)\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-09-12T16:34:59.830256Z","iopub.execute_input":"2024-09-12T16:34:59.831138Z","iopub.status.idle":"2024-09-12T16:35:00.458348Z","shell.execute_reply.started":"2024-09-12T16:34:59.831104Z","shell.execute_reply":"2024-09-12T16:35:00.457438Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**CUSTOM DATASETS** ","metadata":{}},{"cell_type":"code","source":"# Custom dataset to load feature vectors & binary class labels \n\nclass CustomDataset(Dataset):\n    def __init__(self, bags, labels):\n        self.bags = bags\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.bags)\n\n    def __getitem__(self, idx):\n        \n        x = self.bags[idx]\n        if len(x) > 224:\n            feat_map = x[:224]\n        else:\n            feat_map = []\n            feat_map.extend([np.array([0.0] * 224)] * int((224 - len(x)) / 2)) # black padding above\n            feat_map.extend(x) # feature map of patches in between\n            feat_map.extend([np.array([0.0] * 224)] * int(((224 - len(x)) / 2) + 1)) # black padding below\n            feat_map = feat_map[:224]\n        \n        x = np.array(feat_map) \n        \n        x = torch.tensor(x).float()\n        x = nn.functional.normalize(x, dim=0, p=2)  # p=2 for L2 norm, dim=0 for cols\n        bag = x\n        label = self.labels[idx]\n        label = torch.tensor(label)\n\n        return torch.stack([bag]), label","metadata":{"execution":{"iopub.status.busy":"2024-09-12T16:13:17.616046Z","iopub.execute_input":"2024-09-12T16:13:17.616873Z","iopub.status.idle":"2024-09-12T16:13:17.625864Z","shell.execute_reply.started":"2024-09-12T16:13:17.616823Z","shell.execute_reply":"2024-09-12T16:13:17.625013Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create instances of custom datasets\n\ntrain_dataset = CustomDataset(train_bags, train_labels)\nvalid_dataset = CustomDataset(val_bags, val_labels)\ntest_dataset = CustomDataset(test_bags, test_labels)","metadata":{"execution":{"iopub.status.busy":"2024-09-12T16:13:18.622946Z","iopub.execute_input":"2024-09-12T16:13:18.623360Z","iopub.status.idle":"2024-09-12T16:13:18.628051Z","shell.execute_reply.started":"2024-09-12T16:13:18.623332Z","shell.execute_reply":"2024-09-12T16:13:18.627019Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**DATA LOADERS** ","metadata":{}},{"cell_type":"code","source":"batch_size = 16  \n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers = 0)\nvalid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers = 0)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers = 0)","metadata":{"execution":{"iopub.status.busy":"2024-09-12T16:13:20.819980Z","iopub.execute_input":"2024-09-12T16:13:20.820804Z","iopub.status.idle":"2024-09-12T16:13:20.826163Z","shell.execute_reply.started":"2024-09-12T16:13:20.820773Z","shell.execute_reply":"2024-09-12T16:13:20.825245Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2024-09-12T16:13:21.719163Z","iopub.execute_input":"2024-09-12T16:13:21.719983Z","iopub.status.idle":"2024-09-12T16:13:21.724128Z","shell.execute_reply.started":"2024-09-12T16:13:21.719952Z","shell.execute_reply":"2024-09-12T16:13:21.723198Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**FINAL MODEL** ","metadata":{}},{"cell_type":"code","source":"# defining final pipeline to process prepared bags \n\nclass Pipeline(nn.Module):\n    def __init__(self,num_classes):\n        #define necessary layers\n        super().__init__()\n        self.num_classes = num_classes\n          \n        self.base = models.vit_b_32(weights='DEFAULT')\n        # Unfreeze model weights\n        for param in self.base.parameters():\n            param.requires_grad = True\n        \n        self.flatten = nn.Flatten()\n        \n        self.head = nn.Sequential(\n            nn.Linear(1000, 512),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(512, 128),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(128, 4)\n        ) \n    \n    def attention(self, query, key, value, mask=None, dropout=None):\n        \"Compute 'Scaled Dot Product Attention'\"\n        d_k = query.size(-1)\n        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, -1e9)\n        p_attn = scores.softmax(dim=-1)\n        if dropout is not None:\n            p_attn = dropout(p_attn)\n        return torch.matmul(p_attn, value), p_attn\n    \n    def forward(self,X):\n        attended_values, attention_scores = self.attention(X, X, X)\n        X = self.base(attended_values)\n        X = self.flatten(X)\n        X = self.head(X)\n        return X, F.softmax(X)\n\n# modify this depending on the distribution of classes\npos_weight = torch.tensor([1.0, 1.5, 2.5, 0.15])                      # order of weights: benign, in-situ, invasive \npos_weight = pos_weight.to(device)\nloss_fn = nn.CrossEntropyLoss(weight = pos_weight)\nmodel = Pipeline(1)\n#model = nn.DataParallel(model)","metadata":{"execution":{"iopub.status.busy":"2024-09-12T16:24:19.370689Z","iopub.execute_input":"2024-09-12T16:24:19.371467Z","iopub.status.idle":"2024-09-12T16:24:19.565137Z","shell.execute_reply.started":"2024-09-12T16:24:19.371434Z","shell.execute_reply":"2024-09-12T16:24:19.564049Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# modifying first layer for one color channel\n\nmodel.base.features[0][0] = nn.Conv2d(1, 32, kernel_size= (3,3), stride = 2, padding= 1, bias=False)","metadata":{"execution":{"iopub.status.busy":"2024-09-12T16:24:20.390736Z","iopub.execute_input":"2024-09-12T16:24:20.391556Z","iopub.status.idle":"2024-09-12T16:24:20.396700Z","shell.execute_reply.started":"2024-09-12T16:24:20.391525Z","shell.execute_reply":"2024-09-12T16:24:20.395692Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check if GPU is available\nif torch.cuda.is_available():\n    model = model.to('cuda')\n    print('available')\n\n#criterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(params=model.parameters(), lr=0.00001)","metadata":{"execution":{"iopub.status.busy":"2024-09-12T16:24:22.008662Z","iopub.execute_input":"2024-09-12T16:24:22.009434Z","iopub.status.idle":"2024-09-12T16:24:22.039713Z","shell.execute_reply.started":"2024-09-12T16:24:22.009401Z","shell.execute_reply":"2024-09-12T16:24:22.038718Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!export CUDA_LAUNCH_BLOCKING=1","metadata":{"execution":{"iopub.status.busy":"2024-09-12T16:24:22.704872Z","iopub.execute_input":"2024-09-12T16:24:22.705748Z","iopub.status.idle":"2024-09-12T16:24:23.729916Z","shell.execute_reply.started":"2024-09-12T16:24:22.705713Z","shell.execute_reply":"2024-09-12T16:24:23.728608Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**TRAINING** ","metadata":{}},{"cell_type":"code","source":"# Initialize empty lists to store loss and accuracy for training and validation \n\ntrain_losses = []\nvalid_losses = []\ntrain_accuracies = []\nvalid_accuracies = []\nroc_values_train = []\nroc_values_val = []","metadata":{"execution":{"iopub.status.busy":"2024-09-12T16:24:24.836325Z","iopub.execute_input":"2024-09-12T16:24:24.836707Z","iopub.status.idle":"2024-09-12T16:24:24.842559Z","shell.execute_reply.started":"2024-09-12T16:24:24.836677Z","shell.execute_reply":"2024-09-12T16:24:24.841543Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_curve, auc, classification_report\nimport math \n\n# Training loop\nnum_epochs = 20  # Adjust as needed\n\nfor epoch in range(num_epochs):\n    model.train()\n    y_true_train = []\n    y_scores_train = []\n    train_loss = 0\n    correct = 0\n    total = 0\n    for images, labels in tqdm(train_loader):\n        # Move data to GPU if available\n        images = images.to(device)\n        labels = labels.to(device)\n        labels = torch.argmax(labels, dim = 1)\n        \n        optimizer.zero_grad()\n        outputs_without_softmax, outputs_with_softmax = model(images)\n        loss = loss_fn(outputs_without_softmax.squeeze(-1), labels)\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item()  # L+=l.item()\n        predicted = torch.argmax(outputs_with_softmax, dim = 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n        y_true_train.extend(labels.cpu().detach().numpy())\n        y_scores_train.extend(outputs_with_softmax.cpu().detach().numpy())\n\n    train_losses.append(train_loss / len(train_loader))\n    train_accuracies.append(100 * correct / total)\n\n    # roc auc logic\n    #fpr_train, tpr_train, _ = roc_curve(y_true_train, y_scores_train)\n    #roc_auc_train = auc(fpr_train, tpr_train)\n    #roc_values_train.append(roc_auc_train)\n\n    # Plot ROC curve for the training set\n\n    # Validate your model after each epoch if needed\n    model.eval()\n    valid_loss = 0\n    correct = 0\n    total = 0\n    y_true_val = []\n    y_scores_val = []\n    with torch.no_grad():\n        for images, labels in tqdm(valid_loader):\n            # Move data to GPU if available\n            images = images.to(device)\n            labels = labels.to(device)\n            labels = torch.argmax(labels, dim = 1)\n\n            outputs_without_softmax, outputs_with_softmax = model(images)\n            loss = loss_fn(outputs_without_softmax.squeeze(-1), labels)\n            valid_loss += loss.item()  # L+=l.item()\n            predicted = torch.argmax(outputs_with_softmax, dim = 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n            y_true_val.extend(labels.cpu().detach().numpy())\n            y_scores_val.extend(outputs_with_softmax.cpu().detach().numpy())\n\n    valid_losses.append(valid_loss / len(valid_loader))\n    valid_accuracies.append(100 * correct / total)\n    \n    # roc auc logic\n    #fpr_val, tpr_val, _ = roc_curve(y_true_val, y_scores_val)\n    #roc_auc_val = auc(fpr_val, tpr_val)\n    #roc_values_val.append(roc_auc_val)\n\n    # Classification report\n    print(f'Epoch {epoch + 1}, Train Accuracy: {train_accuracies[-1]:.2f}%, Train Loss: {train_losses[-1]:.2f}%, Val Accuracy: {valid_accuracies[-1]:.2f}%, Val Loss: {valid_losses[-1]:.2f}%')\n\n    y_true_val = np.array(y_true_val)\n    y_pred_val = np.array(torch.argmax(torch.tensor(y_scores_val), dim = 1)).astype(int)\n\n    print(\"Validation Classification Report:\")\n    print(classification_report(y_true_val, y_pred_val))","metadata":{"execution":{"iopub.status.busy":"2024-09-12T16:24:25.408477Z","iopub.execute_input":"2024-09-12T16:24:25.408889Z","iopub.status.idle":"2024-09-12T16:25:44.194869Z","shell.execute_reply.started":"2024-09-12T16:24:25.408849Z","shell.execute_reply":"2024-09-12T16:25:44.193912Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def display_training_curves(train_losses, valid_losses, train_accuracies, valid_accuracies, subplot):\n    if subplot % 10 == 1:  # set up the subplots on the first call\n        plt.subplots(figsize=(12, 5), facecolor='#F0F0F0')\n        plt.tight_layout()\n    ax = plt.subplot(subplot)\n    ax.set_facecolor('#F8F8F8')\n    ax.plot(train_losses)\n    ax.plot(valid_losses)\n    ax.set_title('Model Loss')\n    ax.set_ylabel('Loss')\n    ax.set_xlabel('Epoch')\n    ax.legend(['Train', 'Valid'])\n\n    ax = plt.subplot(subplot + 1)\n    ax.set_facecolor('#F8F8F8')\n    ax.plot(train_accuracies)\n    ax.plot(valid_accuracies)\n    ax.set_title('Model Accuracy')\n    ax.set_ylabel('Accuracy')\n    ax.set_xlabel('Epoch')\n    ax.legend(['Train', 'Valid'])","metadata":{"execution":{"iopub.status.busy":"2024-09-12T16:25:58.867892Z","iopub.execute_input":"2024-09-12T16:25:58.868758Z","iopub.status.idle":"2024-09-12T16:25:58.876469Z","shell.execute_reply.started":"2024-09-12T16:25:58.868727Z","shell.execute_reply":"2024-09-12T16:25:58.875424Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot the training curves\ndisplay_training_curves(train_losses, valid_losses, train_accuracies, valid_accuracies, 121)","metadata":{"execution":{"iopub.status.busy":"2024-09-12T16:25:59.313906Z","iopub.execute_input":"2024-09-12T16:25:59.314758Z","iopub.status.idle":"2024-09-12T16:26:00.015107Z","shell.execute_reply.started":"2024-09-12T16:25:59.314723Z","shell.execute_reply":"2024-09-12T16:26:00.014124Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**INFERENCE (BAG LEVEL)** ","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\nmodel.eval()\ntest_loss = 0\ncorrect = 0\ntotal = 0\ny_true_test = []\ny_pred_test = []\ny_probs = []\n\nwith torch.no_grad():\n    for images, labels in tqdm(test_loader):\n        # Move data to GPU if available\n        images = images.to(device)\n        labels = labels.to(device)\n        labels = torch.argmax(labels, dim = 1)\n\n        outputs_without_softmax, outputs_with_softmax = model(images)\n        loss = loss_fn(outputs_without_softmax.squeeze(-1), labels)\n        test_loss += loss.item()  # L+=l.item()\n        predicted = torch.argmax(outputs_with_softmax, dim = 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n        y_true_test.extend(labels.cpu().detach().numpy())\n        y_pred_test.extend(predicted.cpu().detach().numpy())\n        y_probs.extend(outputs_with_softmax.cpu().detach().numpy())\n\ntest_loss /= len(test_loader)\ntest_accuracy = 100 * correct / total\n\n# Calculate ROC-AUC for test set\n#fpr_test, tpr_test, _ = roc_curve(y_true_test, y_probs)\n#roc_auc_test = auc(fpr_test, tpr_test)\n\nprint(f'Test Accuracy: {test_accuracy:.2f}%, Test Loss: {test_loss:.2f}%')\n\n# Classification report for test set\nprint(\"Test Classification Report:\")\nprint(classification_report(y_true_test, y_pred_test))","metadata":{"execution":{"iopub.status.busy":"2024-09-12T16:26:07.553276Z","iopub.execute_input":"2024-09-12T16:26:07.553637Z","iopub.status.idle":"2024-09-12T16:26:07.646762Z","shell.execute_reply.started":"2024-09-12T16:26:07.553609Z","shell.execute_reply":"2024-09-12T16:26:07.645702Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Generate confusion matrix\nconf_matrix = confusion_matrix(y_true_test, y_pred_test)\n\n# Plot confusion matrix\nplt.figure(figsize=(5, 5))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap=\"Blues\", cbar=False)\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.title('Bag Level Confusion Matrix')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-09-12T16:26:07.969732Z","iopub.execute_input":"2024-09-12T16:26:07.970127Z","iopub.status.idle":"2024-09-12T16:26:08.193476Z","shell.execute_reply.started":"2024-09-12T16:26:07.970097Z","shell.execute_reply":"2024-09-12T16:26:08.192525Z"},"trusted":true},"outputs":[],"execution_count":null}]}