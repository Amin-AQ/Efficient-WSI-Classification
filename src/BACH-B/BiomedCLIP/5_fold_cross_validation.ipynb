{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5045636,"sourceType":"datasetVersion","datasetId":2929186},{"sourceId":9611120,"sourceType":"datasetVersion","datasetId":5581965},{"sourceId":9657654,"sourceType":"datasetVersion","datasetId":5899911}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**LOADING DEPENDENCIES**","metadata":{}},{"cell_type":"code","source":"import os\nimport sys\nimport cv2\nimport json\nimport math\nimport torch\nimport random\nimport numpy as np\nimport pandas as pd\nimport openslide\nimport seaborn as sns\nimport tensorflow as tf\nimport torchvision\nimport torch.optim as optim\nimport torch.nn as nn\nfrom tqdm import tqdm\nfrom PIL import Image\nfrom scipy import ndimage\nimport matplotlib.pyplot as plt\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom tqdm.notebook import tqdm\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom torchvision import transforms, datasets\nfrom matplotlib.patches import Polygon\nfrom skimage.transform import rotate, AffineTransform\nfrom matplotlib import patches\nimport xml.etree.ElementTree as ET \nfrom sklearn.model_selection import GroupKFold\nfrom torchvision.transforms import ToTensor\nimport torchvision.models as models\nfrom collections import defaultdict\nfrom sklearn.metrics import roc_curve, auc, classification_report","metadata":{"execution":{"iopub.status.busy":"2024-11-14T18:35:14.722086Z","iopub.execute_input":"2024-11-14T18:35:14.722497Z","iopub.status.idle":"2024-11-14T18:35:14.731288Z","shell.execute_reply.started":"2024-11-14T18:35:14.722456Z","shell.execute_reply":"2024-11-14T18:35:14.730376Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**EXTRACTING PATCHES**","metadata":{}},{"cell_type":"code","source":"# function to return patches along with labels\n\ndef get_patch_info(wsi_nos, save_dir): \n    os.makedirs(save_dir, exist_ok=True)  # Create the directory if it doesn't exist\n    \n    wsi_ids = [] \n    ann_ids = [] \n    patch_paths = []\n    patch_labels = [] \n    \n    for k in range(len(wsi_nos)): \n        print(wsi_nos[k]) \n        \n        # Path to the SVS and XML files\n        img_path = f'/kaggle/input/bach-breast-cancer-histology-images/ICIAR2018_BACH_Challenge/ICIAR2018_BACH_Challenge/WSI/{wsi_nos[k]}.svs'\n        ann_path = f'/kaggle/input/bach-breast-cancer-histology-images/ICIAR2018_BACH_Challenge/ICIAR2018_BACH_Challenge/WSI/{wsi_nos[k]}.xml'\n\n        # Open the SVS file\n        slide = openslide.OpenSlide(img_path)\n\n        # Choosing a level (e.g., 0 for the highest resolution)\n        level = 0\n\n        # Load and parse the XML file\n        tree = ET.parse(ann_path)\n        root = tree.getroot() \n\n        ann_no = 1 \n        # Extract coordinates for each region and group by class\n        for region in root.findall('.//Region'):\n            # Try to get the region_value(class) from the Attribute tag first\n            attribute = region.find('.//Attribute')\n            region_value = attribute.get('Value') if attribute is not None else region.get('Text')\n            \n            # Find region boundaries\n            min_x, max_x, min_y, max_y = float('inf'), float('-inf'), float('inf'), float('-inf')\n            for vertex in region.findall('.//Vertex'): \n                min_x, max_x = min(min_x, float(vertex.get('X'))), max(max_x, float(vertex.get('X')))\n                min_y, max_y = min(min_y, float(vertex.get('Y'))), max(max_y, float(vertex.get('Y')))\n\n            # Calculate width and height of the bounding box\n            box_width = max_x - min_x\n            box_height = max_y - min_y\n\n            # Calculate inner bounding box by ignoring 10% from each side\n            inner_min_x = min_x + 0.1 * box_width\n            inner_max_x = max_x - 0.1 * box_width\n            inner_min_y = min_y + 0.1 * box_height\n            inner_max_y = max_y - 0.1 * box_height\n\n            patch_no = 1\n            # Slide through inner region boundaries for patches\n            for i in range(int(inner_min_y), int(inner_max_y), 500): \n                for j in range(int(inner_min_x), int(inner_max_x), 500): \n                    # Read a region of the slide at the selected level (current patch)\n                    curr_patch = slide.read_region((j, i), level, (500, 500))\n                    curr_patch = curr_patch.resize((224, 224))  \n\n                    # Convert to numpy array\n                    curr_patch = np.array(curr_patch)[:, :, :3]\n                    curr_patch = (curr_patch / 255.0 * 255).astype(np.uint8)  # Normalize to saveable format\n\n                    # Convert to PIL Image for saving\n                    patch_image = Image.fromarray(curr_patch)\n\n                    # Define patch filename\n                    patch_filename = f'{wsi_nos[k]}_ann{ann_no}_patch_no{patch_no}.png'\n                    patch_path = os.path.join(save_dir, patch_filename)\n\n                    # Save the patch image\n                    patch_image.save(patch_path)\n\n                    # Collect patch info\n                    patch_paths.append(patch_path)\n                    patch_labels.append(region_value) \n                    wsi_ids.append(wsi_nos[k]) \n                    ann_ids.append(ann_no) \n                    patch_no += 1\n\n            print('Annotation', ann_no, 'done') \n            ann_no += 1 \n        \n        slide.close() \n    \n    return wsi_ids, ann_ids, patch_paths, patch_labels","metadata":{"execution":{"iopub.status.busy":"2024-11-14T18:38:09.676076Z","iopub.execute_input":"2024-11-14T18:38:09.677053Z","iopub.status.idle":"2024-11-14T18:38:09.693381Z","shell.execute_reply.started":"2024-11-14T18:38:09.677001Z","shell.execute_reply":"2024-11-14T18:38:09.692488Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# unique ids for annotated wsis \n\nids = ['A01', 'A02', 'A03', 'A04', 'A05', 'A06', 'A07', 'A08', 'A09', 'A10'] ","metadata":{"execution":{"iopub.status.busy":"2024-11-14T18:38:12.045783Z","iopub.execute_input":"2024-11-14T18:38:12.046168Z","iopub.status.idle":"2024-11-14T18:38:12.050995Z","shell.execute_reply.started":"2024-11-14T18:38:12.046131Z","shell.execute_reply":"2024-11-14T18:38:12.049917Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# obtaining patch info for all wsis \n\ndir = '/kaggle/working/patches'\nwsi_ids, ann_ids, patch_paths, patch_labels = get_patch_info(ids, dir) ","metadata":{"execution":{"iopub.status.busy":"2024-11-14T18:38:14.647487Z","iopub.execute_input":"2024-11-14T18:38:14.647825Z","iopub.status.idle":"2024-11-14T18:41:13.539812Z","shell.execute_reply.started":"2024-11-14T18:38:14.647794Z","shell.execute_reply":"2024-11-14T18:41:13.538762Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**EXTRACTING NORMAL PATCHES**","metadata":{}},{"cell_type":"code","source":"# function to return patches along with labels\n\ndef get_normal_patch_info(wsi_nos, save_dir, curr_ann_count): \n    os.makedirs(save_dir, exist_ok=True)  # Create the directory if it doesn't exist\n    \n    wsi_ids = [] \n    ann_ids = [] \n    patch_paths = [] \n    patch_labels = []  \n    \n    for k in range(len(wsi_nos)): \n        print(\"WSI \", wsi_nos[k])\n        \n        # Path to the SVS and XML files\n        img_path = '/kaggle/input/bach-breast-cancer-histology-images/ICIAR2018_BACH_Challenge/ICIAR2018_BACH_Challenge/WSI/' + wsi_nos[k] + '.svs' \n        ann_path = '/kaggle/input/bach-dataset-patches/Extra Normal Annotations/Extra Normal Annotations/' + wsi_nos[k] + '_norm.xml' \n\n        # Open the SVS file\n        slide = openslide.OpenSlide(img_path)\n\n        # Choosing a level (e.g., 0 for the highest resolution)\n        level = 0\n\n        # Get slide dimensions at the selected level\n        width, height = slide.level_dimensions[level] \n\n        # Load and parse the XML file\n        tree = ET.parse(ann_path)\n        root = tree.getroot()\n\n        ann_no = curr_ann_count[wsi_nos[k]] + 1\n        \n        # Extract coordinates for each region \n        for region in root.findall('.//Region'): \n            min_x = float('inf')\n            max_x = float('-inf')\n            min_y = float('inf')\n            max_y = float('-inf')\n            for vertex in region.findall('.//Vertex'): \n                # Update min and max x and y coordinates\n                min_x = min(min_x, int(vertex.get('X')))\n                max_x = max(max_x, int(vertex.get('X')))\n                min_y = min(min_y, int(vertex.get('Y')))\n                max_y = max(max_y, int(vertex.get('Y'))) \n\n            # Calculate width and height of the bounding box\n            box_width = max_x - min_x\n            box_height = max_y - min_y\n\n            # Calculate inner bounding box by ignoring 10% from each side\n            inner_min_x = min_x + 0.1 * box_width\n            inner_max_x = max_x - 0.1 * box_width\n            inner_min_y = min_y + 0.1 * box_height\n            inner_max_y = max_y - 0.1 * box_height\n\n            patch_no = 1\n            # Slide vertically within the adjusted bounding box\n            for i in range(int(inner_min_y), int(inner_max_y), 500): \n                # Slide horizontally\n                for j in range(int(inner_min_x), int(inner_max_x), 500): \n\n                    # Read a region of the slide at the selected level (current patch) \n                    curr_patch = slide.read_region((j, i), level, (500, 500)) \n                    curr_patch = curr_patch.resize((224, 224))  \n\n                    # Convert to numpy array\n                    curr_patch = np.array(curr_patch)[:, :, :3]\n                    curr_patch = (curr_patch / 255.0 * 255).astype(np.uint8)  # Normalize to saveable format\n                    \n                    # Convert to PIL Image for saving\n                    patch_image = Image.fromarray(curr_patch)\n\n                    # Define patch filename\n                    patch_filename = f'{wsi_nos[k]}_ann{ann_no}_patch_no{patch_no}.png'\n                    patch_path = os.path.join(save_dir, patch_filename)\n\n                    # Save the patch image\n                    patch_image.save(patch_path)\n\n                    # Collect patch info\n                    patch_paths.append(patch_path)\n                    patch_labels.append(\"Normal\") \n                    wsi_ids.append(wsi_nos[k]) \n                    ann_ids.append(ann_no) \n                    patch_no += 1\n            \n            print('Annotation', ann_no, 'done') \n            ann_no += 1 \n        \n        slide.close() \n    \n    return wsi_ids, ann_ids, patch_paths, patch_labels","metadata":{"execution":{"iopub.status.busy":"2024-11-14T18:41:13.541590Z","iopub.execute_input":"2024-11-14T18:41:13.541905Z","iopub.status.idle":"2024-11-14T18:41:13.557409Z","shell.execute_reply.started":"2024-11-14T18:41:13.541873Z","shell.execute_reply":"2024-11-14T18:41:13.556574Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# checking for consistent lengths\n\nprint(len(wsi_ids), len(ann_ids), len(patch_paths), len(patch_labels))","metadata":{"execution":{"iopub.status.busy":"2024-11-14T19:02:00.147542Z","iopub.execute_input":"2024-11-14T19:02:00.148003Z","iopub.status.idle":"2024-11-14T19:02:00.153537Z","shell.execute_reply.started":"2024-11-14T19:02:00.147959Z","shell.execute_reply":"2024-11-14T19:02:00.152536Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Encode with binary class labels\n\nlabel_to_onehot = {\n    'Benign': 0,\n    'Normal': 0\n}\n\n# Mark 'Benign' and 'Normal' as 0, any other label as 1\npatch_labels = [label_to_onehot.get(label, 1) for label in patch_labels]","metadata":{"execution":{"iopub.status.busy":"2024-11-14T19:02:04.334149Z","iopub.execute_input":"2024-11-14T19:02:04.334545Z","iopub.status.idle":"2024-11-14T19:02:04.340426Z","shell.execute_reply.started":"2024-11-14T19:02:04.334508Z","shell.execute_reply":"2024-11-14T19:02:04.339457Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**CLASS DISTRIBUTION** ","metadata":{}},{"cell_type":"code","source":"# class distribution of train, validation, and test datasets \n\ndef plot_bag_labels(ax, title, labels): \n    \n    # Count occurrences of each class\n    class_counts = [labels.count(i) for i in range(2)]\n    \n    # Plot the distribution\n    ax.bar(['Benign or Normal', 'Insitu Carcinoma or Invasive Carcinoma'], class_counts)\n    ax.set_title(title)\n    ax.set_ylabel('Count')\n\n# Plotting\nfig, axs = plt.subplots(1, 1, figsize=(5, 4))\n\nplot_bag_labels(axs, 'BAG LABELS', patch_labels)\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-11-14T19:02:54.237319Z","iopub.execute_input":"2024-11-14T19:02:54.237999Z","iopub.status.idle":"2024-11-14T19:02:54.481831Z","shell.execute_reply.started":"2024-11-14T19:02:54.237959Z","shell.execute_reply":"2024-11-14T19:02:54.480923Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**CUSTOM DATASETS** ","metadata":{}},{"cell_type":"code","source":"# Defining the custom dataset class\n\nclass CustomDataset(Dataset):\n    def __init__(self, patches, labels):\n        assert len(patches) == len(labels), \"Mismatch in number of patches and labels\"\n        self.patches = patches\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        # Check for valid index range\n        if idx >= len(self.labels) or idx >= len(self.patches):\n            raise IndexError(f\"Index {idx} out of range for dataset with length {len(self)}\")\n\n        patch = Image.open(self.patches[idx]).convert(\"RGB\")  # Adjust this if `patches` are file paths\n        patch = patch.resize((224, 224))  \n        patch = ToTensor()(patch)\n        patch = nn.functional.normalize(patch, dim=0, p=2)\n\n        label = torch.tensor(self.labels[idx], dtype=torch.long)\n        \n        return patch, label","metadata":{"execution":{"iopub.status.busy":"2024-11-14T19:03:39.040832Z","iopub.execute_input":"2024-11-14T19:03:39.041193Z","iopub.status.idle":"2024-11-14T19:03:39.049687Z","shell.execute_reply.started":"2024-11-14T19:03:39.041161Z","shell.execute_reply":"2024-11-14T19:03:39.048604Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2024-11-14T19:03:39.410635Z","iopub.execute_input":"2024-11-14T19:03:39.410971Z","iopub.status.idle":"2024-11-14T19:03:39.415561Z","shell.execute_reply.started":"2024-11-14T19:03:39.410940Z","shell.execute_reply":"2024-11-14T19:03:39.414514Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**FINAL MODEL** ","metadata":{}},{"cell_type":"code","source":"class Pipeline(nn.Module):\n    def __init__(self,num_classes):\n        #define necessary layers\n        super().__init__()\n        self.num_classes = num_classes\n          \n        self.base = models.vit_b_32(weights='DEFAULT')\n\n        # Unfreeze model weights\n        for param in self.base.parameters():\n            param.requires_grad = True\n        \n        self.flatten = nn.Flatten()\n        \n        self.head = nn.Sequential(\n            nn.Linear(1000, 512),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(512, 128),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Linear(64, 1)\n        ) \n    \n    def forward(self,X):\n        X = self.base(X)\n        X = self.flatten(X)\n        X = self.head(X)\n        return X, F.sigmoid(X)\n\npos_weight = torch.tensor([0.6])\npos_weight = pos_weight.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-11-14T19:03:40.295317Z","iopub.execute_input":"2024-11-14T19:03:40.296005Z","iopub.status.idle":"2024-11-14T19:03:40.307314Z","shell.execute_reply.started":"2024-11-14T19:03:40.295965Z","shell.execute_reply":"2024-11-14T19:03:40.306154Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!export CUDA_LAUNCH_BLOCKING=1","metadata":{"execution":{"iopub.status.busy":"2024-11-14T19:03:50.788005Z","iopub.execute_input":"2024-11-14T19:03:50.788834Z","iopub.status.idle":"2024-11-14T19:03:51.802270Z","shell.execute_reply.started":"2024-11-14T19:03:50.788795Z","shell.execute_reply":"2024-11-14T19:03:51.801293Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**K FOLD CROSS VALIDATION** ","metadata":{}},{"cell_type":"code","source":"# Setting some hyperparameters\n\nbatch_size = 16\nnum_epochs = 10\nnum_folds = 5\ngroup_kfold = GroupKFold(n_splits=num_folds)\nmetrics = {\n    fold + 1: {\n        'train_loss': [], 'train_accuracy': [], 'train_roc_auc': [],\n        'valid_loss': [], 'valid_accuracy': [], 'valid_roc_auc': [],\n        'wsi_nos': [], 'ann_nos': [], 'y_pred':[], 'y_true':[]\n    } for fold in range(num_folds)\n}","metadata":{"execution":{"iopub.status.busy":"2024-11-14T19:03:53.483248Z","iopub.execute_input":"2024-11-14T19:03:53.483679Z","iopub.status.idle":"2024-11-14T19:03:53.490637Z","shell.execute_reply.started":"2024-11-14T19:03:53.483640Z","shell.execute_reply":"2024-11-14T19:03:53.489666Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# K Fold cross validation loop\n\nfor fold, (train_idx, test_idx) in enumerate(group_kfold.split(patch_paths, patch_labels, groups=wsi_ids)):\n    \n    # Splitting bags and labels for each fold without converting to numpy array\n    train_patch_paths = [patch_paths[i] for i in train_idx]\n    test_patch_paths = [patch_paths[i] for i in test_idx]\n    train_patch_labels = [patch_labels[i] for i in train_idx]\n    test_patch_labels = [patch_labels[i] for i in test_idx]\n    \n    metrics[fold+1]['wsi_nos'] = [wsi_ids[i] for i in test_idx]\n    metrics[fold+1]['ann_nos'] = [ann_ids[i] for i in test_idx]\n    \n    # Creating datasets and loaders for each fold\n    train_dataset = CustomDataset(train_patch_paths, train_patch_labels)\n    val_dataset = CustomDataset(test_patch_paths, test_patch_labels)\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    valid_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n\n    # Output the fold details\n    print(f\"\\nFold {fold + 1}\")\n    print(f\"Train WSI IDs: {set(np.array(wsi_ids)[train_idx])}\")\n    print(f\"Test WSI IDs: {set(np.array(wsi_ids)[test_idx])}\")\n    print(f\"Train size: {len(train_loader.dataset)}, Test size: {len(valid_loader.dataset)}\")\n    \n    # Initialize empty lists to store loss and accuracy for training and validation\n    train_losses = []\n    valid_losses = []\n    train_accuracies = []\n    valid_accuracies = []\n    roc_values_train = []\n    roc_values_val = []\n    \n    # modifying first layer for one color channel \n    loss_fn = nn.BCEWithLogitsLoss(pos_weight = pos_weight)\n    model = Pipeline(1)\n\n    # Check if GPU is available\n    if torch.cuda.is_available():\n        model = model.to(device)\n        print('available')\n\n    #criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(params=model.parameters(), lr=0.000025)\n    \n    for epoch in range(num_epochs):\n        model.train()\n        y_true_train = []\n        y_scores_train = []\n        train_loss = 0\n        correct = 0\n        total = 0\n        for images, labels in tqdm(train_loader):\n            # Move data to GPU if available\n            images = images.to(device)\n            labels = labels.to(device)\n\n            optimizer.zero_grad()\n            outputs_without_sigmoid, outputs_with_sigmoid = model(images)\n            loss = loss_fn(outputs_without_sigmoid.squeeze(-1), labels.float())\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item()  # L+=l.item()\n            predicted = (outputs_with_sigmoid >= 0.5).long().squeeze(-1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n            y_true_train.extend(labels.cpu().detach().numpy())\n            y_scores_train.extend(outputs_with_sigmoid.cpu().detach().numpy())\n\n        train_losses.append(train_loss / len(train_loader))\n        train_accuracies.append(100 * correct / total)\n\n        # roc auc logic\n        fpr_train, tpr_train, _ = roc_curve(y_true_train, y_scores_train)\n        roc_auc_train = auc(fpr_train, tpr_train)\n        roc_values_train.append(roc_auc_train)\n\n        # Plot ROC curve for the training set\n\n        # Validate your model after each epoch if needed\n        model.eval()\n        valid_loss = 0\n        correct = 0\n        total = 0\n        y_true_val = []\n        y_scores_val = []\n        with torch.no_grad():\n            for images, labels in tqdm(valid_loader):\n                # Move data to GPU if available\n                images = images.to(device)\n                labels = labels.to(device)\n                outputs_without_sigmoid, outputs_with_sigmoid = model(images)\n                loss = loss_fn(outputs_without_sigmoid.squeeze(-1), labels.float())\n                valid_loss += loss.item()  # L+=l.item()\n                predicted = (outputs_with_sigmoid >= 0.5).long().squeeze(-1)\n                total += labels.size(0)\n                correct += (predicted == labels).sum().item()\n\n                y_true_val.extend(labels.cpu().detach().numpy())\n                y_scores_val.extend(outputs_with_sigmoid.cpu().detach().numpy())\n\n        valid_losses.append(valid_loss / len(valid_loader))\n        valid_accuracies.append(100 * correct / total)\n        # roc auc logic\n        fpr_val, tpr_val, _ = roc_curve(y_true_val, y_scores_val)\n        roc_auc_val = auc(fpr_val, tpr_val)\n        roc_values_val.append(roc_auc_val)\n\n        # Classification report\n        print(f'Epoch {epoch + 1}, Train Accuracy: {train_accuracies[-1]:.2f}%, Train Loss: {train_losses[-1]:.2f}%, Train ROC-AUC: {roc_auc_train:.2f}%, Val Accuracy: {valid_accuracies[-1]:.2f}%, Val Loss: {valid_losses[-1]:.2f}%, Val ROC-AUC: {roc_auc_val:.2f}%')\n\n        y_true_val = np.array(y_true_val)\n        y_pred_val = (np.array(y_scores_val) >= 0.5).astype(int)\n        print(\"Validation Classification Report:\")\n        print(classification_report(y_true_val, y_pred_val,zero_division=1))\n        \n         # Append metrics for current epoch and fold\n        metrics[fold + 1]['train_loss'].append(train_losses[-1])\n        metrics[fold + 1]['train_accuracy'].append(train_accuracies[-1])\n        metrics[fold + 1]['train_roc_auc'].append(roc_values_train[-1])\n        metrics[fold + 1]['valid_loss'].append(valid_losses[-1])\n        metrics[fold + 1]['valid_accuracy'].append(valid_accuracies[-1])\n        metrics[fold + 1]['valid_roc_auc'].append(roc_values_val[-1])\n        \n    metrics[fold + 1]['y_pred']=y_pred_val\n    metrics[fold + 1]['y_true']=y_true_val","metadata":{"execution":{"iopub.status.busy":"2024-11-14T19:04:34.612695Z","iopub.execute_input":"2024-11-14T19:04:34.613041Z","iopub.status.idle":"2024-11-14T19:14:20.972010Z","shell.execute_reply.started":"2024-11-14T19:04:34.613008Z","shell.execute_reply":"2024-11-14T19:14:20.971049Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plotting function for losses and accuracies by fold\n\ndef plot_metrics(metrics, num_folds):\n    metric_names = ['train_loss', 'valid_loss', 'train_accuracy', 'valid_accuracy', 'train_roc_auc', 'valid_roc_auc']\n    \n    for metric_name in metric_names:\n        plt.figure(figsize=(10, 5))\n        for fold in range(1, num_folds + 1):\n            fold_metric = metrics[fold][metric_name]\n            plt.plot(range(1, num_epochs + 1), fold_metric, label=f'Fold {fold}')\n        plt.title(f'{metric_name.replace(\"_\", \" \").title()} over Epochs')\n        plt.xlabel('Epoch')\n        plt.ylabel(metric_name.replace('_', ' ').title())\n        plt.legend()\n        plt.show()\n\nplot_metrics(metrics, num_folds)","metadata":{"execution":{"iopub.status.busy":"2024-11-14T19:14:25.328533Z","iopub.execute_input":"2024-11-14T19:14:25.329423Z","iopub.status.idle":"2024-11-14T19:14:26.910510Z","shell.execute_reply.started":"2024-11-14T19:14:25.329382Z","shell.execute_reply":"2024-11-14T19:14:26.909568Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**WSI LEVEL METRICS** ","metadata":{}},{"cell_type":"code","source":"# Function to calculate results on a WSI Level\n\ndef calculate_wsi_level_metrics(wsi_ids, ann_ids, ground_labels, predicted_labels):\n    # Dictionary to store TP, FP, TN, FN counts per WSI and per class\n    wsi_metrics = defaultdict(lambda: {\"0\": {\"TP\": 0, \"FP\": 0, \"TN\": 0, \"FN\": 0}, \n                                       \"1\": {\"TP\": 0, \"FP\": 0, \"TN\": 0, \"FN\": 0}})\n\n    # Iterate through each annotation and count TP, FP, TN, FN per WSI and per class\n    for wsi, ground, pred in zip(wsi_ids, ground_labels, predicted_labels):\n        if ground == 1 and pred == 1:\n            wsi_metrics[wsi][\"1\"][\"TP\"] += 1\n        elif ground == 0 and pred == 1:\n            wsi_metrics[wsi][\"1\"][\"FP\"] += 1\n            wsi_metrics[wsi][\"0\"][\"FN\"] += 1\n        elif ground == 0 and pred == 0:\n            wsi_metrics[wsi][\"0\"][\"TP\"] += 1\n        elif ground == 1 and pred == 0:\n            wsi_metrics[wsi][\"1\"][\"FN\"] += 1\n            wsi_metrics[wsi][\"0\"][\"FP\"] += 1\n\n    # Calculate accuracy for class 1 and F1 scores for each class per WSI\n    wsi_scores = {}\n    for wsi, metrics in wsi_metrics.items():\n        # Calculate accuracy for class 1\n        total_1 = metrics[\"1\"][\"TP\"] + metrics[\"1\"][\"FP\"] + metrics[\"1\"][\"TN\"] + metrics[\"1\"][\"FN\"]\n        accuracy_1 = ((metrics[\"1\"][\"TP\"] + metrics[\"1\"][\"TN\"]) / total_1) * 100 if total_1 > 0 else 0\n\n        # Calculate F1 scores for both classes\n        f1_scores = {}\n        for class_label in [\"0\", \"1\"]:\n            tp, fp, tn, fn = metrics[class_label][\"TP\"], metrics[class_label][\"FP\"], metrics[class_label][\"TN\"], metrics[class_label][\"FN\"]\n\n            precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n            recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n            f1_scores[class_label] = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n\n        wsi_scores[wsi] = (accuracy_1, f1_scores[\"0\"], f1_scores[\"1\"])\n\n    for wsi, (accuracy_1, f1_0, f1_1) in wsi_scores.items():\n        print(f\"WSI {wsi}: Class 1 Accuracy = {accuracy_1:.2f}%, F1 Score Class 0 = {f1_0:.2f}, F1 Score Class 1 = {f1_1:.2f}\")","metadata":{"execution":{"iopub.status.busy":"2024-11-14T19:14:30.981122Z","iopub.execute_input":"2024-11-14T19:14:30.981826Z","iopub.status.idle":"2024-11-14T19:14:30.995340Z","shell.execute_reply.started":"2024-11-14T19:14:30.981782Z","shell.execute_reply":"2024-11-14T19:14:30.994376Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Displaying WSI level metrics\n\nwsi_level_metrics = {}\n\nfor fold in range(1, num_folds + 1):\n    # Retrieve WSI IDs, annotation IDs, true labels, and predictions for the validation data in the current fold\n    wsi_ids = metrics[fold]['wsi_nos']\n    ann_ids = metrics[fold]['ann_nos']\n    true_labels = metrics[fold]['y_true']\n    predicted_labels = metrics[fold]['y_pred']\n\n    wsi_metrics = calculate_wsi_level_metrics(wsi_ids, ann_ids, true_labels, predicted_labels)\n    wsi_level_metrics[fold] = wsi_metrics\n","metadata":{"execution":{"iopub.status.busy":"2024-11-14T19:14:34.462792Z","iopub.execute_input":"2024-11-14T19:14:34.463148Z","iopub.status.idle":"2024-11-14T19:14:34.477265Z","shell.execute_reply.started":"2024-11-14T19:14:34.463115Z","shell.execute_reply":"2024-11-14T19:14:34.476225Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}