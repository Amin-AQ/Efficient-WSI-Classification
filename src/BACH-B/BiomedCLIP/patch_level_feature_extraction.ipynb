{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5045636,"sourceType":"datasetVersion","datasetId":2929186},{"sourceId":9611120,"sourceType":"datasetVersion","datasetId":5581965},{"sourceId":9657654,"sourceType":"datasetVersion","datasetId":5899911}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# importing necessary libraries \n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nimport torchvision \nfrom torchvision import transforms, datasets, models \nfrom torch.nn import functional as F \nfrom PIL import Image\nimport pandas as pd \nimport numpy as np\nimport tensorflow as tf\nimport os\nimport sys\nimport glob\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt \nimport cv2\nimport json \nfrom tqdm import tqdm\nfrom sklearn.decomposition import PCA \nfrom sklearn.preprocessing import StandardScaler\nfrom skimage.transform import rotate, AffineTransform\nimport random\nfrom scipy import ndimage\nimport openslide\nimport matplotlib.patches as patches\nfrom matplotlib.patches import Polygon\nimport xml.etree.ElementTree as ET ","metadata":{"execution":{"iopub.status.busy":"2024-09-28T05:21:54.546400Z","iopub.execute_input":"2024-09-28T05:21:54.546860Z","iopub.status.idle":"2024-09-28T05:22:14.605372Z","shell.execute_reply.started":"2024-09-28T05:21:54.546804Z","shell.execute_reply":"2024-09-28T05:22:14.604193Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**EXTRACTING PATCH EMBEDDINGS** ","metadata":{}},{"cell_type":"code","source":"# feature extractor for patches \n\nclass Resnet50(nn.Module):\n    def __init__(self,num_classes):\n        #define necessary layers\n        super().__init__()\n        self.num_classes = num_classes\n        self.model = models.resnet50(weights=True)\n        \n        # Unfreeze model weights\n        for param in self.model.parameters():\n            param.requires_grad = False \n        \n    def forward(self,X):\n        #define forward pass here\n        X = self.model(X)\n        return X        \n            \nmodel = Resnet50(1)","metadata":{"execution":{"iopub.status.busy":"2024-09-28T05:22:14.607345Z","iopub.execute_input":"2024-09-28T05:22:14.607996Z","iopub.status.idle":"2024-09-28T05:22:16.310396Z","shell.execute_reply.started":"2024-09-28T05:22:14.607963Z","shell.execute_reply":"2024-09-28T05:22:16.309248Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Transform function for patches \n\ntransform = torchvision.transforms.Compose(\n    [\n        torchvision.transforms.Resize((224, 224)),  # resizing to 224x224\n        torchvision.transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),  # normalization\n    ]\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-28T05:22:16.312235Z","iopub.execute_input":"2024-09-28T05:22:16.312579Z","iopub.status.idle":"2024-09-28T05:22:16.319474Z","shell.execute_reply.started":"2024-09-28T05:22:16.312549Z","shell.execute_reply":"2024-09-28T05:22:16.318173Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# function to obtain feature embedding for a given patch \n\ndef get_feature_vector(img): \n    img = torch.from_numpy(img.astype(np.double)) \n    img = img.permute(2, 0, 1) \n    img = transform(img) \n    img = img.float() \n    img = torch.unsqueeze(img, dim=0) \n    return model(img) ","metadata":{"execution":{"iopub.status.busy":"2024-09-28T05:22:16.322918Z","iopub.execute_input":"2024-09-28T05:22:16.323699Z","iopub.status.idle":"2024-09-28T05:22:16.332561Z","shell.execute_reply.started":"2024-09-28T05:22:16.323658Z","shell.execute_reply":"2024-09-28T05:22:16.331412Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# function to obtain the percentage of while area in a given patch \n\n# threshold for labelling a pixel as white \nthresh = 200 \n\n# mask for visualization of white area in a patch \nmask = np.zeros((224, 224, 3), dtype = np.uint8)\n\ndef get_perc_white_area(img): \n    \n    count = 0 \n    white = 0 \n    \n    i = 0 \n    for row in img: \n        j = 0 \n        for index in row: \n\n            count = count + 1 \n            rgb = 0.299 * index[0] + 0.587 * index[1] + 0.114 * index[2] \n            if rgb > thresh: \n                white = white + 1 \n                \n            j += 1\n        i += 1\n\n    return (white / count) * 100","metadata":{"execution":{"iopub.status.busy":"2024-09-28T05:22:16.333682Z","iopub.execute_input":"2024-09-28T05:22:16.334020Z","iopub.status.idle":"2024-09-28T05:22:16.344480Z","shell.execute_reply.started":"2024-09-28T05:22:16.333990Z","shell.execute_reply":"2024-09-28T05:22:16.343237Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# function to return feature vectors along with labels for patches \n\ndef get_patch_embeddings(wsi_nos): \n    \n    wsi_ids = [] \n    ann_ids = [] \n    feature_vectors = [] \n    labels = [] \n    white_area_percentages = [] \n    x_pixels = [] \n    y_pixels = [] \n    \n    for k in range(len(wsi_nos)): \n        \n        print(wsi_nos[k]) \n        \n        # Path to the SVS and XML files\n        img_path = '/kaggle/input/bach-breast-cancer-histology-images/ICIAR2018_BACH_Challenge/ICIAR2018_BACH_Challenge/WSI/' + wsi_nos[k] + '.svs' \n        ann_path = '/kaggle/input/bach-breast-cancer-histology-images/ICIAR2018_BACH_Challenge/ICIAR2018_BACH_Challenge/WSI/' + wsi_nos[k] + '.xml' \n\n        # Open the SVS file\n        slide = openslide.OpenSlide(img_path)\n\n        # Choosing a level (e.g., 0 for the highest resolution)\n        level = 0\n\n        # Get slide dimensions at the selected level\n        width, height = slide.level_dimensions[level] \n\n        # Load and parse the XML file\n        tree = ET.parse(ann_path)\n        root = tree.getroot() \n\n        ann_no = 1 \n        # Extract coordinates for each region and group by class\n        for region in root.findall('.//Region'):\n            # Try to get the region_value(class) from the Attribute tag first\n            attribute = region.find('.//Attribute')\n            if attribute is not None:\n                region_value = attribute.get('Value')\n            else:\n                # If no Attribute is found, fall back to the Text attribute in the Region tag\n                region_value = region.get('Text') \n            \n            min_x = float('inf')\n            max_x = float('-inf')\n            min_y = float('inf')\n            max_y = float('-inf') \n            \n            for vertex in region.findall('.//Vertex'): \n                \n                # Update min and max x and y coordinates\n                min_x = min(min_x, float(vertex.get('X')))\n                max_x = max(max_x, float(vertex.get('X')))\n                min_y = min(min_y, float(vertex.get('Y')))\n                max_y = max(max_y, float(vertex.get('Y'))) \n            \n            # loop to slide vertically for patches \n            for i in range(int(min_y), int(max_y), 384): \n\n                # loop to slide horizontally \n                for j in range(int(min_x), int(max_x), 384): \n\n                    # Read a region of the slide at the selected level (current patch) \n                    curr_patch = slide.read_region((j, i), level, (512, 512)) \n\n                    # Convert to numpy array \n                    curr_patch = np.array(curr_patch) \n                    curr_patch = curr_patch[:, :, :3] \n                    \n                    # Normalization (remove in case of issues with results) \n                    #curr_patch /= 255.0 \n                    \n                    # Storing percentage of white area \n                    white_area_perc = get_perc_white_area(curr_patch) \n                    white_area_percentages.append(white_area_perc) \n\n                    # Storing feature vector \n                    feature_vector = get_feature_vector(curr_patch) \n                    squeezed_arr = np.squeeze(feature_vector) \n                    \n                    feature_vectors.append(squeezed_arr) \n\n                    # Storing labels \n                    labels.append(region_value) \n\n                    # Storing WSI Id \n                    wsi_ids.append(k) \n\n                    # Storing annotation Id \n                    ann_ids.append(ann_no) \n                    \n                    # Storing starting pixel values \n                    x_pixels.append(j) \n                    y_pixels.append(i) \n            \n            print('annotation', ann_no, 'done') \n            \n            ann_no += 1 \n\n        # Close the slide\n        slide.close() \n    \n    return wsi_ids, ann_ids, feature_vectors, white_area_percentages, x_pixels, y_pixels, labels ","metadata":{"execution":{"iopub.status.busy":"2024-09-28T05:22:16.346069Z","iopub.execute_input":"2024-09-28T05:22:16.346404Z","iopub.status.idle":"2024-09-28T05:22:16.366688Z","shell.execute_reply.started":"2024-09-28T05:22:16.346376Z","shell.execute_reply":"2024-09-28T05:22:16.365487Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# unique ids for annotated wsis \n\nids = ['A01', 'A02', 'A03', 'A04', 'A05', 'A06', 'A07', 'A08', 'A09', 'A10'] ","metadata":{"execution":{"iopub.status.busy":"2024-09-28T05:22:16.368243Z","iopub.execute_input":"2024-09-28T05:22:16.368715Z","iopub.status.idle":"2024-09-28T05:22:16.381601Z","shell.execute_reply.started":"2024-09-28T05:22:16.368677Z","shell.execute_reply":"2024-09-28T05:22:16.380410Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# obtaining patch embeddings for all wsis \n\nwsi_ids, ann_ids, patch_embeddings, white_area_percentages, x_loc, y_loc, patch_labels = get_patch_embeddings(ids) ","metadata":{"execution":{"iopub.status.busy":"2024-09-28T05:22:16.383184Z","iopub.execute_input":"2024-09-28T05:22:16.383675Z","iopub.status.idle":"2024-09-28T05:22:45.906596Z","shell.execute_reply.started":"2024-09-28T05:22:16.383630Z","shell.execute_reply":"2024-09-28T05:22:45.905113Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**SAVING PATCH LEVEL DATA & EMBEDDINGS** ","metadata":{}},{"cell_type":"code","source":"# storing patch info in a pandas dataframe \n\npatch_info_df = pd.DataFrame({ \n                'WSI Id' : wsi_ids, \n                'Annotation No.' : ann_ids, \n                'X Start Pixel': x_loc, \n                'Y Start Pixel': y_loc, \n                'White Area %': white_area_percentages, \n                'Label': patch_labels \n}) ","metadata":{"execution":{"iopub.status.busy":"2024-08-22T10:49:50.286211Z","iopub.status.idle":"2024-08-22T10:49:50.286664Z","shell.execute_reply.started":"2024-08-22T10:49:50.286465Z","shell.execute_reply":"2024-08-22T10:49:50.286484Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"patch_info_df.head() ","metadata":{"execution":{"iopub.status.busy":"2024-08-22T10:49:50.288800Z","iopub.status.idle":"2024-08-22T10:49:50.289330Z","shell.execute_reply.started":"2024-08-22T10:49:50.289055Z","shell.execute_reply":"2024-08-22T10:49:50.289078Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# saving csv file \n\ncsv_file_path = '/kaggle/working/bach_patch_embeddings_info.csv' \npatch_info_df.to_csv(csv_file_path, index=False) ","metadata":{"execution":{"iopub.status.busy":"2024-08-22T10:49:50.291252Z","iopub.status.idle":"2024-08-22T10:49:50.291799Z","shell.execute_reply.started":"2024-08-22T10:49:50.291501Z","shell.execute_reply":"2024-08-22T10:49:50.291525Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# storing patch embeddings in pth file \n\ntorch.save(patch_embeddings,'bach_patch_embeddings.pth') ","metadata":{"execution":{"iopub.status.busy":"2024-08-22T10:49:50.292980Z","iopub.status.idle":"2024-08-22T10:49:50.293498Z","shell.execute_reply.started":"2024-08-22T10:49:50.293228Z","shell.execute_reply":"2024-08-22T10:49:50.293251Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}